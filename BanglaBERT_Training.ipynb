{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3585,"status":"ok","timestamp":1662003431738,"user":{"displayName":"Mukta's Cuisine","userId":"09477569564665203851"},"user_tz":-360},"id":"RefOmAryZn5C","outputId":"d3eae370-df54-4fbe-dd01-a48546ecef8c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.7/dist-packages (4.21.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (1.21.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.8.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.12.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.64.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.9.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2022.6.2)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.12.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2.23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (21.3)\n","Requirement already satisfied: protobuf<=3.20.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.17.3)\n","Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.1.97)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers[sentencepiece]) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers[sentencepiece]) (3.0.9)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1->transformers[sentencepiece]) (1.15.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers[sentencepiece]) (3.8.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2022.6.15)\n"]}],"source":["!pip install transformers[sentencepiece]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3890,"status":"ok","timestamp":1662003435623,"user":{"displayName":"Mukta's Cuisine","userId":"09477569564665203851"},"user_tz":-360},"id":"A1wtsxoXZrGG","outputId":"4ac4f605-6b76-4433-d18c-45ac6e1973e7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/csebuetnlp/normalizer\n","  Cloning https://github.com/csebuetnlp/normalizer to /tmp/pip-req-build-e0hrw0gm\n","  Running command git clone -q https://github.com/csebuetnlp/normalizer /tmp/pip-req-build-e0hrw0gm\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from normalizer==0.0.1) (2022.6.2)\n","Requirement already satisfied: emoji==1.4.2 in /usr/local/lib/python3.7/dist-packages (from normalizer==0.0.1) (1.4.2)\n","Requirement already satisfied: ftfy==6.0.3 in /usr/local/lib/python3.7/dist-packages (from normalizer==0.0.1) (6.0.3)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy==6.0.3->normalizer==0.0.1) (0.2.5)\n"]}],"source":["pip install git+https://github.com/csebuetnlp/normalizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6363,"status":"ok","timestamp":1662003441983,"user":{"displayName":"Mukta's Cuisine","userId":"09477569564665203851"},"user_tz":-360},"id":"V6iWegerEFPC","outputId":"e961f65e-5377-4550-ea5e-641ae6526730"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at csebuetnlp/banglabert were not used when initializing ElectraForTokenClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight']\n","- This IS expected if you are initializing ElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at csebuetnlp/banglabert and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import AutoModelForTokenClassification, AutoTokenizer\n","from normalizer import normalize # pip install git+https://github.com/csebuetnlp/normalizer\n","import torch\n","\n","model = AutoModelForTokenClassification.from_pretrained(\"csebuetnlp/banglabert\", num_labels=3)\n","tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/banglabert\", use_fast=True,)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lRCVoMSFrXAn"},"outputs":[],"source":["model.config.architectures = ['ElectraForTokenClassification']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3257,"status":"ok","timestamp":1662003445229,"user":{"displayName":"Mukta's Cuisine","userId":"09477569564665203851"},"user_tz":-360},"id":"mwINPgAaI3oY","outputId":"a5f62d2e-14f2-4e4d-8b6c-3b43e668482f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Mounting colab with Google Drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F1ULsSSOI5N7"},"outputs":[],"source":["# It it recommended to not change the dataset's files' name\n","\n","# 'base' variable has to store the path to the folder where datasets are uploaded/available\n","# so set the 'base' variable path to the folder of the where you have uploaded the datasets\n","base = '/content/drive/MyDrive/BQA'        # sample\n","\n","# 'checkpoint_dir' variable has to store the path to the folder where checkpoints will be saves\n","# IT IS RECOMMENDED TO SET checkpoint_dir SAME AS base\n","# so set the 'checkpoint_dir' variable path to the folder of the where you want the check points to be saved\n","checkpoint_dir = '/content/drive/MyDrive/BQA'          #sample\n","\n","# 'train_filename' variable has to store the filename of the Train set\n","# so set the 'train_filename' variable as the name of the Train set's filename\n","train_filename = 'Train.json'        #sample\n","\n","# 'validation_file_name' variable has to store the filename of the Validation set\n","# so set the 'validation_file_name' variable as the name of the validation set's filename\n","validation_file_name = 'Validation.json'     #sample"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1488,"status":"ok","timestamp":1662003446712,"user":{"displayName":"Mukta's Cuisine","userId":"09477569564665203851"},"user_tz":-360},"id":"aquaIZo2I8ks","outputId":"339ffda2-f143-400f-c02e-f4621d7082da"},"outputs":[{"data":{"text/plain":["dict_keys(['data'])"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["# Loading the Train.json file for train set of BanglaRQA\n","\n","import json\n","import os\n","\n","f = open(os.path.join(base,train_filename))\n","  \n","data_test = json.load(f)\n","\n","data_test.keys()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1662003446713,"user":{"displayName":"Mukta's Cuisine","userId":"09477569564665203851"},"user_tz":-360},"id":"N2uyoMMxI-_i","outputId":"47a08479-288a-412f-aadf-4ed2902330c3"},"outputs":[{"data":{"text/plain":["2400"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["data = data_test['data']\n","len(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tdqr3P7SJCOD"},"outputs":[],"source":["import string, re\n","def remove_punc(text):\n","  exclude = set(string.punctuation)\n","  exclude.remove('?')\n","  return \"\".join(ch for ch in text if ch not in exclude)\n","\n","context = []\n","question = []\n","answer = []\n","answer_raw = []\n","\n","for i in range(len(data)):\n","    for j in range(len(data[i]['qas'])):\n","        context.append(remove_punc(normalize(data[i]['context'])))\n","        question.append(remove_punc(normalize(data[i]['qas'][j]['question_text'])))\n","        answer.append(normalize(data[i]['qas'][j]['answers']['answer_text'][0]))\n","        answer_raw.append(normalize(data[i]['qas'][j]['answers']['answer_text'][0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1662003477433,"user":{"displayName":"Mukta's Cuisine","userId":"09477569564665203851"},"user_tz":-360},"id":"poiU6Xu5JDpG","outputId":"4f591f44-255f-431b-ed0b-d42a971f38d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["মোট কত বছর ব্যাপী আলিয়া মাদ্রাসার পূর্ণাঙ্গ আধুনিক শিক্ষা ব্যবস্থা প্রবর্তন করা হয় ?\n","ফাজিল পরীক্ষা বাংলাদেশ ও ভারতের আলিয়া মাদ্রাসায় অনুষ্ঠিত একটি সরকারি পরীক্ষা। ফাজিল পরীক্ষা বাংলাদেশে ডিগ্রি সমমানের কখনো স্নাতক সমমানের একটি পরীক্ষা যা একটি ফাজিল মাদ্রাসায় অনুষ্ঠিত হয়ে থাকে। তবে ভারতে ফাজিল পরীক্ষাকে উচ্চ মাধ্যমিক শ্রেণীর ১১ বা ১২ ক্লাস মান বলে বিবেচিত করা হয়। ফাজিল পরীক্ষা বাংলাদেশ ভারত ও পাকিস্তানের সরকারি স্বীকৃত আলিয়া মাদরাসায় প্রচলিত রয়েছে। বাংলাদেশের ফাজিল পরীক্ষা ইসলামি আরবি বিশ্ববিদ্যালয়ের অধীনে অনুষ্ঠিত হয়ে থাকে ও ভারতের ফাজিল পরীক্ষা পশ্চিমবঙ্গ মাদ্রাসা শিক্ষা পর্ষদের অধীনে অনুষ্ঠিত হয়ে থাকে। ১৯৪৭ সালে ঢাকা আলিয়া মাদ্রাসা ঢাকায় স্থানান্তরের পূর্বে বাংলাদেশ ও ভারতের ফাজিল পরীক্ষা কলকাতা আলিয়া মাদ্রাসার অধীনে অনুষ্ঠিত হতো। ফাযিল পরীক্ষা বর্তমানে ইসলামি আরবী বিশ্ববিদ্যালয়ের অধীনে অনুষ্ঠিত হয়। যা পূর্বে মাদরাসা বোর্ড ও ইসলামি বিশ্ববিদ্যালয়ের আধীনে অনুষ্ঠিত হত। মাদ্রাসাইআলিয়া ঢাকায় স্থানান্তরিত হলে ১৯৪৮ সালে মাদ্রাসা বোর্ডের ফাজিলগুলো পরীক্ষা ঢাকা বিশ্ববিদ্যালয় কর্তৃক গৃহীত হতো। ১৯৭৫ সালের কুদরতএখুদা শিক্ষা কমিশনের সুপারিশে মাদ্রাসা বোর্ড নিয়ন্ত্রিত আলিয়া মাদ্রাসাসমূহে জাতীয় শিক্ষাক্রম ও বহুমুখী পাঠ্যসূচি প্রবর্তিত করা হয়। ১৯৮০ সালে অনুষ্ঠিত ফাজিল পরীক্ষায় এই পাঠ্যসুচী কার্যকর হয়। এই শিক্ষা কমিশন অনুসারে ফাজিল শ্রেণীতে ইসলামি শিক্ষার পাশাপাশি সাধারণ পাঠ্যসূচী অন্তর্ভুক্ত করে ফাজিল পরীক্ষাকে সাধারণ উচ্চ মাধ্যমিক এইচ এস সির সমমান ঘোষণা করা হয়। ১৯৭৮ সালে অধ্যাপক মুস্তফা বিন কাসিমের নেতৃত্বে সিনিয়র মাদ্রাসা শিক্ষা ব্যবস্থা কমিটি গঠিত হয়। এই কমিটির নির্দেশনায় ১৯৮৪ সালে সাধারণ শিক্ষার স্তরের সঙ্গে বাংলাদেশ মাদ্রাসা বোর্ড নিয়ন্ত্রিত আলিয়া মাদ্রাসা শিক্ষা স্তরের সামঞ্জস্য করা হয়। ফাজিল স্তরকে ২ বছর মেয়াদী কোর্সে উন্নিত করে মোট ১৬ বছর ব্যাপী আলিয়া মাদ্রাসার পূর্ণাঙ্গ আধুনিক শিক্ষা ব্যবস্থা প্রবর্তন করা হয়। এই কমিশনের মাধ্যমেই সরকার ফাজিল পরীক্ষাকে সাধারণ ডিগ্রি মান ঘোষণা করে।\n","11912\n","11912\n","11912\n","11912\n"]}],"source":["print(question[4])\n","print(context[0])\n","print(len(context))\n","print(len(question))\n","print(len(answer))\n","print(len(answer_raw))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1662003477434,"user":{"displayName":"Mukta's Cuisine","userId":"09477569564665203851"},"user_tz":-360},"id":"8C_d0VW9FEt8","outputId":"72c1bdba-0423-4ac5-ffaf-680704bfee33"},"outputs":[{"name":"stdout","output_type":"stream","text":["ফাজিল পরীক্ষা বাংলাদেশ ও ভারতের আলিয়া মাদ্রাসায় অনুষ্ঠিত একটি সরকারি পরীক্ষা । ফাজিল পরীক্ষা বাংলাদেশে ডিগ্রি সমমানের কখনো স্নাতক সমমানের একটি পরীক্ষা যা একটি ফাজিল মাদ্রাসায় অনুষ্ঠিত হয়ে থাকে । তবে ভারতে ফাজিল পরীক্ষাকে উচ্চ মাধ্যমিক শ্রেণীর ১১ বা ১২ ক্লাস মান বলে বিবেচিত করা হয় । ফাজিল পরীক্ষা বাংলাদেশ ভারত ও পাকিস্তানের সরকারি স্বীকৃত আলিয়া মাদরাসায় প্রচলিত রয়েছে । বাংলাদেশের ফাজিল পরীক্ষা ইসলামি আরবি বিশ্ববিদ্যালয়ের অধীনে অনুষ্ঠিত হয়ে থাকে ও ভারতের ফাজিল পরীক্ষা পশ্চিমবঙ্গ মাদ্রাসা শিক্ষা পর্ষদের অধীনে অনুষ্ঠিত হয়ে থাকে । ১৯৪৭ সালে ঢাকা আলিয়া মাদ্রাসা ঢাকায় স্থানান্তরের পূর্বে বাংলাদেশ ও ভারতের ফাজিল পরীক্ষা কলকাতা আলিয়া মাদ্রাসার অধীনে অনুষ্ঠিত হতো । ফাযিল পরীক্ষা বর্তমানে ইসলামি আরবী বিশ্ববিদ্যালয়ের অধীনে অনুষ্ঠিত হয় । যা পূর্বে মাদরাসা বোর্ড ও ইসলামি বিশ্ববিদ্যালয়ের আধীনে অনুষ্ঠিত হত । মাদ্রাসাইআলিয়া ঢাকায় স্থানান্তরিত হলে ১৯৪৮ সালে মাদ্রাসা বোর্ডের ফাজিলগুলো পরীক্ষা ঢাকা বিশ্ববিদ্যালয় কর্তৃক গৃহীত হতো । ১৯৭৫ সালের কুদরতএখুদা শিক্ষা কমিশনের সুপারিশে মাদ্রাসা বোর্ড নিয়ন্ত্রিত আলিয়া মাদ্রাসাসমূহে জাতীয় শিক্ষাক্রম ও বহুমুখী পাঠ্যসূচি প্রবর্তিত করা হয় । ১৯৮০ সালে অনুষ্ঠিত ফাজিল পরীক্ষায় এই পাঠ্যসুচী কার্যকর হয় । এই শিক্ষা কমিশন অনুসারে ফাজিল শ্রেণীতে ইসলামি শিক্ষার পাশাপাশি সাধারণ পাঠ্যসূচী অন্তর্ভুক্ত করে ফাজিল পরীক্ষাকে সাধারণ উচ্চ মাধ্যমিক এইচ এস সির সমমান ঘোষণা করা হয় । ১৯৭৮ সালে অধ্যাপক মুস্তফা বিন কাসিমের নেতৃত্বে সিনিয়র মাদ্রাসা শিক্ষা ব্যবস্থা কমিটি গঠিত হয় । এই কমিটির নির্দেশনায় ১৯৮৪ সালে সাধারণ শিক্ষার স্তরের সঙ্গে বাংলাদেশ মাদ্রাসা বোর্ড নিয়ন্ত্রিত আলিয়া মাদ্রাসা শিক্ষা স্তরের সামঞ্জস্য করা হয় । ফাজিল স্তরকে ২ বছর মেয়াদী কোর্সে উন্নিত করে মোট ১৬ বছর ব্যাপী আলিয়া মাদ্রাসার পূর্ণাঙ্গ আধুনিক শিক্ষা ব্যবস্থা প্রবর্তন করা হয় । এই কমিশনের মাধ্যমেই সরকার ফাজিল পরীক্ষাকে সাধারণ ডিগ্রি মান ঘোষণা করে ।\n"]}],"source":["for i in range(len(context)):\n","  context[i] = context[i].replace('।', ' ।')\n","  answer[i] = answer[i].replace('।', ' ।')\n","  question[i] = question[i].replace('।', ' ।')\n","\n","print(context[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IT7IDNDcpu6h"},"outputs":[],"source":["for i in range(len(context)):\n","  if question[i].find('?') == -1:\n","    question[i] = question[i] + '?'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IJG4CcANpJZt"},"outputs":[],"source":["for i in range(len(context)):\n","  if question[i].find(' ?') == -1:\n","    question[i] = question[i].replace('?', ' ?')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1662003477435,"user":{"displayName":"Mukta's Cuisine","userId":"09477569564665203851"},"user_tz":-360},"id":"V9LK_G-LoqNR","outputId":"55ab0491-6c1a-4fd7-9821-7af0901b1a2f"},"outputs":[{"name":"stdout","output_type":"stream","text":["11912\n"]}],"source":["c = 0\n","for i in range(len(context)):\n","  if question[i].find(' ?') != -1:\n","    c = c+1\n","print(c)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QhRH9-h-R-TV"},"outputs":[],"source":["question_context = []\n","\n","for i in range(len(context)):\n","  context[i] = 'হ্যাঁ না । ' + context[i]\n","  question_context.append(question[i] + ' ' + context[i])\n","  a = question[i] + ' ' + context[i]\n","  ans_spans = answer[i].split('; ')\n","\n","  for ans in ans_spans:\n","    ans = remove_punc(ans)\n","    if(ans == ''):\n","      continue\n","    else:\n","      ansx = ans.split()\n","      x = ''\n","      for j in range(len(ansx)):\n","        if(j == 0):\n","          x = 'B'\n","        else:\n","          x = x + ' I'\n","      a = a.replace(ans, x, 1)\n","\n","  a = a.split()\n","  for j, x in enumerate(a):\n","    if(x != 'B' and x!= 'I'):\n","      a[j] = 'O'\n","\n","  answer[i] = a\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1662003478110,"user":{"displayName":"Mukta's Cuisine","userId":"09477569564665203851"},"user_tz":-360},"id":"sH9dT9ZbO9N6","outputId":"40bb9402-e6a7-409a-d25d-1f0dfd53fbd6"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'B': 0, 'I': 1, 'O': 2}\n","{0: 'B', 1: 'I', 2: 'O'}\n"]}],"source":["labels_to_ids = {k: v for v, k in enumerate(['B', 'I', 'O'])}\n","ids_to_labels = {v: k for v, k in enumerate(['B', 'I', 'O'])}\n","print(labels_to_ids)\n","print(ids_to_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1662003478111,"user":{"displayName":"Mukta's Cuisine","userId":"09477569564665203851"},"user_tz":-360},"id":"006f8ju1JK6P","outputId":"e7388584-1b4d-4078-d6bc-eb5329936414"},"outputs":[{"data":{"text/plain":["dict_keys(['data'])"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["# Loading the validation set of BanglaRQA\n","\n","import json\n","import os\n","\n","f_val = open(os.path.join(base,validation_file_name))\n","  \n","data_val = json.load(f_val)\n","\n","data_val.keys()\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dfSHKxKJK1c"},"outputs":[],"source":["data_val = data_val['data']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F_mqb5Z1JUBx"},"outputs":[],"source":["context_val = []\n","question_val = []\n","answer_val = []\n","\n","for i in range(len(data_val)):\n","    for j in range(len(data_val[i]['qas'])):\n","        context_val.append(remove_punc(normalize(data_val[i]['context'])))\n","        question_val.append(remove_punc(normalize(data_val[i]['qas'][j]['question_text'])))\n","        answer_val.append(normalize(data_val[i]['qas'][j]['answers']['answer_text'][0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1662003481743,"user":{"displayName":"Mukta's Cuisine","userId":"09477569564665203851"},"user_tz":-360},"id":"JGIRUjxwqlpU","outputId":"a253d48a-879f-44eb-8c43-adf20718fbab"},"outputs":[{"name":"stdout","output_type":"stream","text":["1484\n","আস্যাইরিয়ান ও ব্যাবলিয়ান সাহিত্যে সরগন তার অবনমিত অবস্থান থেকে ক্ষমতায় উত্থান ও মেসোপটেমিয়া অভিযানের জন্য কিংবদন্তি গল্পের মুল উপজীব্যে পরিনত হয়েছিলেন এরকম আরও কিছু কিছু আংশিক কিংবদন্তী উপাখ্যান ছাড়াও সারগনের খোদ নিজের অনেক লিপি আছে যদিও তার বেশিরভাগ পরবর্তী সংস্করনগুলি থেকে নেওয়া। ল্যুভর জাদুঘরে দুটি সারগনিক বিজয় ফলক এর অংশবিশেষ আছে যা সুসা যেখানে এগুলি খুব সম্ভবত মেসোপটেমিয়া ত্থেকে দ্বাদশ শতাব্দীতে স্থানান্তরিত করা হয়েছিল থেকে পুনঃউদ্ধার করা হয়েছিল। দৃশ্যত সরগন সেমেটিক আক্কাদীয়ান ভাষার লিপির লিখিত আকারে প্রসার ঘটিয়েছিলেন তিনি আক্কাদ শহর প্রতিষ্ঠিত করার প্রথমদিকে নিজেকে প্রায়শয়ই আক্কাদীয়ান রাজা হিসিবে প্রচার করতেন পরে তিনি কোন এক সময় কিস শহর অধিগ্রহণ করে নেন পরবর্তীতে মেসোপটেমিয়ার বৃহদাংশও দখল করে নেন এবং ক্রমে ক্রমে সরগন আক্কদীয়ান রাজা ইনান্নার তত্ত্বাবধায়ক কিস এর রাজা আনুর স্থলাভিষিক্ত রাজ্যেরমেসোপটেমিয়া রাজা এনলিলের রাজ্যপালএনসি নামে নিজেকে প্রচার করে ছিলেন। যদিও সুমেরিয়ান রাজাদের তালিকার অনেক অনুলিপিতে সারগনের শাসনকাল ৫৬ ৫৫ বা ৫৪ বছর বলা হয়েছে কিন্তু তারিখ সংবলিত নথি অনুযায়ী তার আসল শাসনকালের শুধুমাত্র ৪ টি ভিন্ন ভিন্ন বছরের উল্লেখ পাওয়া যায় এই চার শাসনবছরে তিনি এলাম মারি সিমুররামএকটি হুঋয়ান অঞ্চল এবং উরুয়ার একটি এলামিটে নগররাষ্ট্র বিরুদ্ধে অভিযান পরিচালনা করেন বলে বিবরন পাওয়া যায়। সারগনের শাসনামলে কীলকাকার লিপির জন্য পূর্ব সেমেটিক ভাষাকে প্রমিতকরণ ও স্বীকৃতকরণ করা হয়েছিল যা পূর্বে সুমেরিয়ান ভাষার সাথে ব্যবহৃত হত এবং বর্তমানের আক্কাদীয়ান ভাষা হিসেবে প্রচলিত।মাটির ফলক ও স্তম্ভে পুরাণ এবং ধর্মীয় আচারঅনুষ্ঠানের দৃশ্য প্রথিত করার এক শিল্পস্বরুপহস্তলিপি ক্যালিগ্রাফি এই সময়ে বিকশিত হয়েছিল।\n"]}],"source":["print(len(context_val))\n","print(context_val[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1662003481743,"user":{"displayName":"Mukta's Cuisine","userId":"09477569564665203851"},"user_tz":-360},"id":"WAsUeQb9GxBQ","outputId":"1108621b-d67b-4317-d75a-72e804f778af"},"outputs":[{"name":"stdout","output_type":"stream","text":["আস্যাইরিয়ান ও ব্যাবলিয়ান সাহিত্যে সরগন তার অবনমিত অবস্থান থেকে ক্ষমতায় উত্থান ও মেসোপটেমিয়া অভিযানের জন্য কিংবদন্তি গল্পের মুল উপজীব্যে পরিনত হয়েছিলেন এরকম আরও কিছু কিছু আংশিক কিংবদন্তী উপাখ্যান ছাড়াও সারগনের খোদ নিজের অনেক লিপি আছে যদিও তার বেশিরভাগ পরবর্তী সংস্করনগুলি থেকে নেওয়া । ল্যুভর জাদুঘরে দুটি সারগনিক বিজয় ফলক এর অংশবিশেষ আছে যা সুসা যেখানে এগুলি খুব সম্ভবত মেসোপটেমিয়া ত্থেকে দ্বাদশ শতাব্দীতে স্থানান্তরিত করা হয়েছিল থেকে পুনঃউদ্ধার করা হয়েছিল । দৃশ্যত সরগন সেমেটিক আক্কাদীয়ান ভাষার লিপির লিখিত আকারে প্রসার ঘটিয়েছিলেন তিনি আক্কাদ শহর প্রতিষ্ঠিত করার প্রথমদিকে নিজেকে প্রায়শয়ই আক্কাদীয়ান রাজা হিসিবে প্রচার করতেন পরে তিনি কোন এক সময় কিস শহর অধিগ্রহণ করে নেন পরবর্তীতে মেসোপটেমিয়ার বৃহদাংশও দখল করে নেন এবং ক্রমে ক্রমে সরগন আক্কদীয়ান রাজা ইনান্নার তত্ত্বাবধায়ক কিস এর রাজা আনুর স্থলাভিষিক্ত রাজ্যেরমেসোপটেমিয়া রাজা এনলিলের রাজ্যপালএনসি নামে নিজেকে প্রচার করে ছিলেন । যদিও সুমেরিয়ান রাজাদের তালিকার অনেক অনুলিপিতে সারগনের শাসনকাল ৫৬ ৫৫ বা ৫৪ বছর বলা হয়েছে কিন্তু তারিখ সংবলিত নথি অনুযায়ী তার আসল শাসনকালের শুধুমাত্র ৪ টি ভিন্ন ভিন্ন বছরের উল্লেখ পাওয়া যায় এই চার শাসনবছরে তিনি এলাম মারি সিমুররামএকটি হুঋয়ান অঞ্চল এবং উরুয়ার একটি এলামিটে নগররাষ্ট্র বিরুদ্ধে অভিযান পরিচালনা করেন বলে বিবরন পাওয়া যায় । সারগনের শাসনামলে কীলকাকার লিপির জন্য পূর্ব সেমেটিক ভাষাকে প্রমিতকরণ ও স্বীকৃতকরণ করা হয়েছিল যা পূর্বে সুমেরিয়ান ভাষার সাথে ব্যবহৃত হত এবং বর্তমানের আক্কাদীয়ান ভাষা হিসেবে প্রচলিত ।মাটির ফলক ও স্তম্ভে পুরাণ এবং ধর্মীয় আচারঅনুষ্ঠানের দৃশ্য প্রথিত করার এক শিল্পস্বরুপহস্তলিপি ক্যালিগ্রাফি এই সময়ে বিকশিত হয়েছিল ।\n"]}],"source":["for i in range(len(context_val)):\n","  context_val[i] = context_val[i].replace('।', ' ।')\n","  answer_val[i] = answer_val[i].replace('।', ' ।')\n","  question_val[i] = question_val[i].replace('।', ' ।')\n","\n","print(context_val[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FFbQFGXbqdEK"},"outputs":[],"source":["for i in range(len(context_val)):\n","  if question_val[i].find('?') == -1:\n","    question_val[i] = question_val[i] + '?'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O5YcOt_qnmsw"},"outputs":[],"source":["for i in range(len(question_val)):\n","  if question_val[i].find(' ?') == -1:\n","    question_val[i] = question_val[i].replace('?', ' ?')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1662003481744,"user":{"displayName":"Mukta's Cuisine","userId":"09477569564665203851"},"user_tz":-360},"id":"MkfbJbR_q6P4","outputId":"e81702dd-df89-45f6-dc3b-d71aaa9ac6c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["1484\n"]}],"source":["c = 0\n","for i in range(len(context_val)):\n","  if question_val[i].find(' ?') != -1:\n","    c = c+1\n","print(c)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1662003481744,"user":{"displayName":"Mukta's Cuisine","userId":"09477569564665203851"},"user_tz":-360},"id":"9mgHtjb6ohBt","outputId":"5b313a98-f3a0-4364-9412-5fa14c4e7f6a"},"outputs":[{"name":"stdout","output_type":"stream","text":["কোন জাদুঘরে সারগনিক বিজয় ফলক আছে ?\n"]}],"source":["print(question_val[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bGYwIJYBJT9Z"},"outputs":[],"source":["from collections import Counter\n","\n","# these functions are heavily influenced by the HF squad_metrics.py script\n","def normalize_text(s):\n","    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n","    import string, re\n","\n","    def remove_articles(text):\n","        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n","        return re.sub(regex, \" \", text)\n","\n","    def white_space_fix(text):\n","        return \" \".join(text.split())\n","\n","    def remove_punc(text):\n","        exclude = set(string.punctuation)\n","        return \"\".join(ch for ch in text if ch not in exclude)\n","\n","    def lower(text):\n","        return text.lower()\n","\n","    return white_space_fix(remove_articles(remove_punc(lower(s))))\n","\n","def compute_exact_match(prediction, truth):\n","    return int(normalize_text(prediction) == normalize_text(truth))\n","\n","def compute_f1(prediction, truth):\n","    pred_tokens = normalize_text(prediction).split()\n","    truth_tokens = normalize_text(truth).split()\n","    \n","    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n","    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n","        return int(pred_tokens == truth_tokens)\n","    \n","    common_tokens = Counter(pred_tokens) & Counter(truth_tokens)\n","    common_tokens = sum(common_tokens.values())\n","    \n","    # if there are no common tokens then f1 = 0\n","    if common_tokens == 0:\n","        return 0\n","    \n","    prec = 1.0 * common_tokens / len(pred_tokens)\n","    rec = 1.0 * common_tokens / len(truth_tokens)\n","    \n","    return 2 * (prec * rec) / (prec + rec)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TMf0j3lHM6xZ"},"outputs":[],"source":["label_all_tokens = False\n","\n","def align_label(qc, labels):\n","    #print(qc)\n","    #print(labels)\n","    #tokenized_inputs = tokenizer(q, c, max_length=512, padding=\"max_length\", truncation=\"only_second\", return_attention_mask=True, add_special_tokens=False, return_tensors=\"pt\")\n","    \n","    tokenized_inputs = tokenizer(qc, max_length=512, padding=\"max_length\", truncation=True, return_attention_mask=True, add_special_tokens=True, return_tensors=\"pt\")\n","    \n","    word_ids = tokenized_inputs.word_ids()\n","    #print(word_ids)\n","    previous_word_idx = None\n","    label_ids = []\n","\n","    for word_idx in word_ids:\n","        #print(word_idx)\n","        if word_idx is None:\n","            label_ids.append(-100)\n","\n","        elif word_idx != previous_word_idx:\n","            try:\n","                label_ids.append(labels_to_ids[labels[word_idx]])\n","            except:\n","                label_ids.append(-100)\n","        else:\n","            try:\n","                label_ids.append(labels_to_ids[labels[word_idx]] if label_all_tokens else -100)\n","            except:\n","                label_ids.append(-100)\n","        \n","        #print(label_ids)\n","        previous_word_idx = word_idx\n","\n","    return label_ids\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o0h4mB5Rp6G5"},"outputs":[],"source":["ins = []\n","outs = []\n","for i in range(len(context)):\n","  ins.append(tokenizer(question_context[i], max_length=512, padding=\"max_length\", truncation=True, return_attention_mask=True, add_special_tokens=True, return_tensors=\"pt\"))\n","  outs.append(align_label(question_context[i], answer[i]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6T4bc5Z7lFRc"},"outputs":[],"source":["class DataSequence(torch.utils.data.Dataset):\n","\n","    def __init__(self, i, o):\n","\n","        #lb = [i.split() for i in a]\n","        self.texts = i\n","        self.labels = o\n","\n","    def __len__(self):\n","\n","        return len(self.labels)\n","\n","    def get_batch_data(self, idx):\n","\n","        return self.texts[idx]\n","\n","    def get_batch_labels(self, idx):\n","\n","        return torch.LongTensor(self.labels[idx])\n","\n","    def __getitem__(self, idx):\n","\n","        batch_data = self.get_batch_data(idx)\n","        batch_labels = self.get_batch_labels(idx)\n","\n","        return batch_data, batch_labels\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yOdNy-NBr94G"},"outputs":[],"source":["train_dataset = DataSequence(ins, outs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MlWJTcSdwEX3"},"outputs":[],"source":["def align_word_ids(qc):\n","  \n","    tokenized_inputs = tokenizer(qc, max_length=512, padding=\"max_length\", truncation=True, return_attention_mask=True, add_special_tokens=True, return_tensors=\"pt\")\n","    #print(qc)\n","    word_ids = tokenized_inputs.word_ids()\n","    #print(word_ids)\n","    previous_word_idx = None\n","    label_ids = []\n","\n","    for word_idx in word_ids:\n","\n","        if word_idx is None:\n","            label_ids.append(-100)\n","\n","        elif word_idx != previous_word_idx:\n","            try:\n","                label_ids.append(1)\n","            except:\n","                label_ids.append(-100)\n","        else:\n","            try:\n","                label_ids.append(1 if label_all_tokens else -100)\n","            except:\n","                label_ids.append(-100)\n","        previous_word_idx = word_idx\n","\n","    return label_ids\n","\n","\n","def evaluate_one_text(qc):\n","\n","    text = tokenizer(qc, max_length=512, padding=\"max_length\", truncation=True, return_attention_mask=True, add_special_tokens=True, return_tensors=\"pt\")\n","    \n","    mask = text['attention_mask'].to(device)\n","    input_id = text['input_ids'].to(device)\n","    label_ids = torch.Tensor(align_word_ids(qc)).unsqueeze(0).to(device)\n","\n","    logits = model(input_id, mask, None)\n","    #print(logits[0])\n","    #print(len(logits[0][0]))\n","    #logits_clean = logits[0][label_ids != -100]\n","    #print(logits_clean)\n","    #print(logits[0][0])\n","\n","    predictions = logits[0][0].argmax(dim=1).tolist()\n","    #print(predictions)\n","    prediction_label = [ids_to_labels[i] for i in predictions]\n","    #print(prediction_label)\n","\n","    #print(input_id)\n","    a = tokenizer.convert_ids_to_tokens(input_id[0])\n","    #print(a)\n","    '''\n","    print(prediction_label)\n","    print(len(prediction_label))\n","    print(a)\n","    print(len(a))\n","    '''\n","\n","    ans = ''\n","    ind = True\n","    for i in range(len(prediction_label)):\n","      #print(i)\n","      if(prediction_label[i] == 'B' and ind == True):\n","        ans = ans + a[i]\n","        ind = False\n","      elif (prediction_label[i] == 'B'):\n","        ans = ans + '; ' + a[i]\n","      elif (label_ids[0][i] == -100 and prediction_label[i] == 'I'):\n","        ans = ans + a[i]\n","      elif (prediction_label[i] == 'I'):\n","        ans = ans + ' ' + a[i]\n","\n","    return ans"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"95FLsxPiJcP_","outputId":"284e78fc-8706-4803-ce96-33c90734e83e"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","Epoch 0: 100%|██████████| 1489/1489 [06:13<00:00,  3.98it/s, loss=0.0242]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 0\n","F1:  0.6910454483535459\n","EM:  0.4636118598382749\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1: 100%|██████████| 1489/1489 [06:10<00:00,  4.02it/s, loss=0.0114]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1\n","F1:  0.723513589008439\n","EM:  0.5154986522911051\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2: 100%|██████████| 1489/1489 [06:11<00:00,  4.01it/s, loss=0.0224]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2\n","F1:  0.7323755102209044\n","EM:  0.534366576819407\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3: 100%|██████████| 1489/1489 [06:11<00:00,  4.01it/s, loss=0.00539]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3\n","F1:  0.7129627071236639\n","EM:  0.5336927223719676\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4: 100%|██████████| 1489/1489 [06:11<00:00,  4.01it/s, loss=0.00572]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4\n","F1:  0.6848322058059423\n","EM:  0.5047169811320755\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5: 100%|██████████| 1489/1489 [06:12<00:00,  4.00it/s, loss=0.00693]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5\n","F1:  0.7251259522735782\n","EM:  0.5411051212938005\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 6: 100%|██████████| 1489/1489 [06:11<00:00,  4.01it/s, loss=0.00438]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6\n","F1:  0.6945775867292739\n","EM:  0.5262803234501348\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 7: 100%|██████████| 1489/1489 [06:09<00:00,  4.03it/s, loss=0.0022]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7\n","F1:  0.7326560207684922\n","EM:  0.5296495956873315\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 8: 100%|██████████| 1489/1489 [06:09<00:00,  4.03it/s, loss=0.00549]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8\n","F1:  0.7104916899125873\n","EM:  0.5168463611859838\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 9: 100%|██████████| 1489/1489 [06:09<00:00,  4.03it/s, loss=0.00202]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9\n","F1:  0.7142064311622461\n","EM:  0.5175202156334232\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 10: 100%|██████████| 1489/1489 [06:09<00:00,  4.03it/s, loss=0.00785]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10\n","F1:  0.6889242753952048\n","EM:  0.5006738544474394\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 11: 100%|██████████| 1489/1489 [06:09<00:00,  4.03it/s, loss=0.00569]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 11\n","F1:  0.7026511742187506\n","EM:  0.49528301886792453\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 12: 100%|██████████| 1489/1489 [06:09<00:00,  4.03it/s, loss=0.00324]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 12\n","F1:  0.6630456905346483\n","EM:  0.49326145552560646\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 13: 100%|██████████| 1489/1489 [06:09<00:00,  4.03it/s, loss=0.00106]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 13\n","F1:  0.7060186034075248\n","EM:  0.5101078167115903\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 14: 100%|██████████| 1489/1489 [06:11<00:00,  4.01it/s, loss=0.00232]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 14\n","F1:  0.7042134235892009\n","EM:  0.5020215633423181\n"]}],"source":["# Training part \n","# no of epoch = 15\n","# opimizer = Adam\n","# at every epoch EM and F1 scores were calculated on validation set of BanglaRQA\n","# at every epoch model was also saved\n","\n","\n","from torch.utils.data import DataLoader\n","from transformers import AdamW\n","from tqdm import tqdm\n","\n","# setup GPU/CPU\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","# move model over to detected device\n","model.to(device)\n","\n","\n","# activate training mode of model\n","model.train()\n","# initialize adam optimizer with weight decay (reduces chance of overfitting)\n","optim = AdamW(model.parameters(), lr=2e-5)\n","\n","# initialize data loader for training data\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","\n","for epoch in range(15):\n","    # set model to train mode\n","    model.train()\n","    model_name = 'banglaBERT_model_weights_epoch_' + str(epoch) + '.pth'\n","    # setup loop (we use tqdm for the progress bar)\n","    loop = tqdm(train_loader, leave=True)\n","    for train_data, train_label in loop:\n","        # initialize calculated gradients (from prev step)\n","        \n","        \n","\n","        optim.zero_grad()\n","        # pull all the tensor batches required for training\n","        input_ids = train_data['input_ids'].squeeze(1).to(device)\n","        attention_mask = train_data['attention_mask'].squeeze(1).to(device)\n","        labels = train_label.to(device)\n","      \n","        outputs = model(input_ids=input_ids, \n","                          attention_mask=attention_mask, \n","                            labels = labels)\n","        #print(outputs)\n","        # extract loss\n","        loss = outputs[0]\n","        #print(loss)\n","        # calculate loss for every parameter that needs grad update \n","        loss.backward()\n","        # update parameters\n","        optim.step()\n","\n","        #print(outputs)\n","        #print(model.config.architectures)\n","        # print relevant info to progress bar\n","        loop.set_description(f'Epoch {epoch}')\n","        loop.set_postfix(loss=loss.item())\n","\n","    torch.save({\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optim.state_dict(),\n","        'loss': loss,\n","        \n","        }, os.path.join(checkpoint_dir,model_name))\n","    \n","    optim.zero_grad()\n","    model.eval()\n","\n","\n","    f1_total = 0.0\n","    em_total = 0.0\n","\n","    with torch.no_grad():\n","      l = len(context_val)\n","      for i in range(l):\n","          pred = evaluate_one_text(question_val[i] + ' হ্যাঁ না । ' + context_val[i])\n","          pred = pred.replace('#', '')\n","          #if(i<10):\n","            #print(pred)\n","          f1_total = f1_total + compute_f1(pred, answer_val[i])\n","          em_total = em_total + compute_exact_match(pred, answer_val[i])\n","\n","      print(\"Epoch\", epoch)\n","      print('F1: ', f1_total/l)\n","      print('EM: ', em_total/l)  "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"YpIxpQXHrIIC","outputId":"a57f9881-4cb5-43e6-837f-6289d7cc74f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","হ্যাঁ\n","হ্যাঁ\n","1\n","\n","\n","2\n","১৯৪৭\n","১৯৪৭\n","3\n","১৯৪৮\n","১৯৪৮\n","4\n","১৬\n","১৬\n","5\n","\n","\n","6\n","একটি জাপানি মাঙ্গা সিরিজ\n","একটি জাপানি মাঙ্গা সিরিজ\n","7\n","অনু লড়াইয়ের দানব\n","অনু. লড়াইয়ের দানব\n","8\n","এই প্রাচীর টাইটান নামে অভিহিত বিশাল আকৃতির মানুষ খাওয়া হিউম্যানয়েডদের থেকে তাদেরকে রক্ষা করে\n","এই প্রাচীর টাইটান নামে অভিহিত বিশাল আকৃতির মানুষ খাওয়া হিউম্যানয়েডদের থেকে তাদেরকে রক্ষা করে\n","9\n","\n","\n","10\n"]}],"source":["with torch.no_grad():\n","      l = len(context_val)\n","      for i in range(l):\n","          print(i)\n","          pred = evaluate_one_text(question_context[i])\n","          pred = pred.replace('#', '')\n","          if(i<10):\n","            print(pred)\n","            print(answer_raw[i])\n","          else:\n","            break"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"oDnI_QSWCd58","outputId":"e7a9d8b3-3f24-4eab-acb5-4d0c755e933b"},"outputs":[{"name":"stdout","output_type":"stream","text":["!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"]}],"source":["print(string.punctuation)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"FeVTU-g5GHwJ","outputId":"ec25379d-b317-49d6-8ff6-31758cad22ec"},"outputs":[{"data":{"text/plain":["False"]},"execution_count":67,"metadata":{},"output_type":"execute_result"}],"source":["'।' == '|'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Sb9Mf-C1GUal"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"provenance":[],"authorship_tag":"ABX9TyPmFQbF63JKi+71g4xnaChL"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}